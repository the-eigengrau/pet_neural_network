{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Required for Step 1 - Convolution\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Required for Step 2 - Pooling \n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# Required for Step 3 - Flattening\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# Required for Step 4 - Full connection ANN - You have already used this\n",
    "from keras.layers import Dense\n",
    "\n",
    "seed=47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(_file_name='config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "experiment_name = 'Autonomyous-Vehicle-Stop-Sign-Classifier'\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.keras import log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28374 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = datagen.flow_from_directory('./GTSRB_Train/Final_Training/Images',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 subset = 'training',\n",
    "                                                 seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7092 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = datagen.flow_from_directory('./GTSRB_Train/Final_Training/Images',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            # class_mode = 'binary',\n",
    "                                            subset = 'validation',\n",
    "                                            seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialize the CNN\n",
    "neural = Sequential()\n",
    "\n",
    "#Convolution and Pooling Layers\n",
    "neural.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "neural.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "neural.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "neural.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "neural.add(Flatten())\n",
    "\n",
    "#Hidden Layers\n",
    "neural.add(Dense(units = 128, activation = 'relu'))\n",
    "neural.add(Dropout(0.5))\n",
    "neural.add(Dense(units = 128, activation = 'relu'))\n",
    "neural.add(Dropout(0.5))\n",
    "neural.add(Dense(units = 128, activation = 'relu'))\n",
    "neural.add(Dropout(0.5))\n",
    "neural.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "#Output Layer\n",
    "neural.add(Dense(units = 37, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Early Stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(patience=3, monitor='loss', min_delta=0.001)\n",
    "\n",
    "## Callbacks can be a list\n",
    "clf_callbacks = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.keras import log_model\n",
    "\n",
    "def neuralRunner(run_name, compile_arguments, fit_arguments):\n",
    "\n",
    "  with mlflow.start_run(run_name=run_name) as run:\n",
    "    model = neural\n",
    "    model.compile(**compile_arguments)\n",
    "    \n",
    "    history = model.fit_generator(**fit_arguments)\n",
    "    \n",
    "    for param_key, param_value in {**compile_arguments, **fit_arguments}.items():\n",
    "      if param_key not in [\"x\", \"y\"]:\n",
    "        mlflow.log_param(param_key, param_value)\n",
    "    \n",
    "    for key, values in history.history.items():\n",
    "      for i, v in enumerate(values):\n",
    "        mlflow.log_metric(key, v, step=i)\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "      mlflow.log_param(f\"hidden_layer_{i}_units\", layer.output_shape)\n",
    "      \n",
    "    log_model(model, \"stop_classifier\")\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to use categorical cross-entropy loss\n",
    "compile_arguments = {\n",
    "  \"optimizer\": \"adam\", \n",
    "  \"loss\": \"categorical_crossentropy\",\n",
    "  \"metrics\": [\"accuracy\"],\n",
    "}\n",
    "\n",
    "# Specify arguments to fit the Keras classifier model - it would look like below\n",
    "# classifier.fit_generator(training_set, steps_per_epoch = 8000, epochs = 8, \n",
    "# validation_data = test_set, validation_steps = 2000, callbacks=clf_callbacks)\n",
    "fit_arguments = {\n",
    "  \"generator\": training_set, \n",
    "  \"steps_per_epoch\": 221,\n",
    "  \"epochs\": 10,\n",
    "  \"validation_data\": test_set,\n",
    "  \"validation_steps\": 2000, \n",
    "  \"callbacks\": clf_callbacks,\n",
    "  \"verbose\": 2,\n",
    "  \"use_multiprocessing\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07162020_131344\n"
     ]
    }
   ],
   "source": [
    "## Import Python datetime to create a runname that includes the datetime when it was started\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "print(datetime_str)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML run name:  AV_stop_classifier07162020_131344\n"
     ]
    }
   ],
   "source": [
    "ml_run_name = \"AV_stop_classifier\" + datetime_str\n",
    "print(\"ML run name: \", ml_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 2.0268 - accuracy: 0.3186 - val_loss: 1.8624 - val_accuracy: 0.3503\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.9616 - accuracy: 0.3281 - val_loss: 1.8229 - val_accuracy: 0.3589\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.9180 - accuracy: 0.3406 - val_loss: 1.8010 - val_accuracy: 0.3835\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.9080 - accuracy: 0.3470 - val_loss: 1.7704 - val_accuracy: 0.3466\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.8352 - accuracy: 0.3634 - val_loss: 1.6839 - val_accuracy: 0.3718\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.7894 - accuracy: 0.3808 - val_loss: 1.6612 - val_accuracy: 0.4157\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.7584 - accuracy: 0.3808 - val_loss: 1.6993 - val_accuracy: 0.3474\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.7494 - accuracy: 0.3821 - val_loss: 1.6886 - val_accuracy: 0.3823\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.7163 - accuracy: 0.3978 - val_loss: 1.6208 - val_accuracy: 0.4199\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 - 22s - loss: 1.7159 - accuracy: 0.4070 - val_loss: 1.6341 - val_accuracy: 0.4213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/07/16 13:18:08 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under azureml://experiments/Autonomyous-Vehicle-Stop-Sign-Classifier/runs/45f7fd75-5b08-434d-9549-9569371070e3/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n"
     ]
    }
   ],
   "source": [
    "run_name = ml_run_name\n",
    "run = neuralRunner(run_name, compile_kwargs, fit_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
